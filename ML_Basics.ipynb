{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77503213-2e5a-4ba5-bb14-12306fb0c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ee070d-da78-46bb-808c-6518a9088831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxopt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a5d672f-3108-44ed-b797-371f5ab7850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "        self.y = None\n",
    "        self.classes = None\n",
    "    def fit(self, X, y):\n",
    "        self.y = y\n",
    "        self.classes = np.unique(y)\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_where_c = X[np.where(self.y==c)]\n",
    "            self.parameters.append([])\n",
    "            for col in X_where_c.T:\n",
    "                parameters = {\"mean\": col.mean(), \"var\": col.var()}\n",
    "                self.parameters[i].append(parameters)\n",
    "    \n",
    "    def _calculate_prior(self, c):\n",
    "        frequency = np.mean(self.y == c)\n",
    "        return frequency\n",
    "    def _calculate_likelihood(self, mean, var, X):\n",
    "        eps = 1e-4\n",
    "        coeff = 1.0 / math.sqrt(2.0 * math.pi * var + eps)\n",
    "        exponent = math.exp(-(math.pow(X-mean, 2) / (2 * var + eps)))\n",
    "        return coeff * exponent\n",
    "    \n",
    "    def _calculate_probabilities(self, X):\n",
    "        posteriors = []\n",
    "        for i, c in enumerate(self.classes):\n",
    "            posteriors = self._calculate_prior(c)\n",
    "            for feature_value, params in zip(X, self.parameters[i]):\n",
    "                likelihood = self._calculate_likelihood(params[\"mean\"], params[\"var\"], feature_value)\n",
    "                posterior *= likelihood\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.sum(y == y_pred, axis = 0 ) / len(y)\n",
    "        return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bc1374-5e05-4b36-8e24-7c460983a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def Sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self, learning_rate = .1):\n",
    "        self.params = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigmoid = Sigmoid\n",
    "        \n",
    "    def _initialize_parameters(self, X):\n",
    "        n_features = np.shape(X)[1]\n",
    "        limit = 1 / math.sqrt(n_features)\n",
    "        self.param = np.random.uniform(-limit, limit, (n_features,))\n",
    "        \n",
    "    def fit(self, X, y, n_iterations=4000):\n",
    "        self._initianize_parameters(X)\n",
    "        for i in range(n_iterations):\n",
    "            y_pred = self.sigmoid(X.dot(self.param))\n",
    "            self.param -= self.learning_rate * -(y- y_pred).dot(X)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = self.sigmoid(X.dot(self.param))\n",
    "        return y_pred\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.sum(y==y_pred, axis=0) / len(y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce66543-ee00-490c-bbe7-4b05cc7124b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SupportVectorMachine\n",
    "cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "def linear_kernel(**kwargs):\n",
    "    def f(x1, x2):\n",
    "        return np.inner(x1, x2)\n",
    "    return f\n",
    "\n",
    "def polynomial_kernel(power, coef, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        return (np.inner(x1, x2) + coef)**power\n",
    "    return f\n",
    "\n",
    "def rbf_kernel(gamma, **kwargs):\n",
    "    def f(x1, x2):\n",
    "        distance = np.linalg.norm(x1-x2)**2\n",
    "        return np.exp(-gamma*distance)\n",
    "    return f\n",
    "\n",
    "class SupportVectorMachine():\n",
    "    def __init__(self, kernel = linear_kernel, power=4, gamma=None, coef=4):\n",
    "        self.kernel = kernel\n",
    "        self.power = power \n",
    "        self.gamma = gamma \n",
    "        self.coef = coef\n",
    "        self.lagr_multipliers = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_labels = None\n",
    "        self.intercept = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        \n",
    "        if not self.gamma:\n",
    "            self.gamma = 1 / n_features\n",
    "            \n",
    "        self.kernel = self.kernel(\n",
    "            power = self.power,\n",
    "            gamma = self.gamma,\n",
    "            coef = self.coef)\n",
    "        \n",
    "        kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                kernel_matrix[i, j] = self.kernel(X[i], X[j])\n",
    "                \n",
    "        p = cvxopt.matrix(np.outer(y, y) * kernel_matrix, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(n_samples)* -1)\n",
    "        A = cvxopt.matrix(y, (1, n_samples), tc = 'd')\n",
    "        b = cvxopt.matrix(0, tc='d')\n",
    "        \n",
    "        G = cvxopt.matrix(np.identity(n_samples)*-1)\n",
    "        h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        \n",
    "        minimization = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        lagr_mult = np.ravel(minimization['x'])\n",
    "        idx = lagr_multipliers = lagr_mult[idx]\n",
    "        self.support_vectors = X[idx]\n",
    "        self.support_vectors_labels = y[idx]\n",
    "        \n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_multipliers[i] * self.support_vector_labels[i] * \\\n",
    "            self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "            \n",
    "        def predict(self, X):\n",
    "            y_pred = []\n",
    "            for sample in X:\n",
    "                prediction = 0\n",
    "                for i in range(len(self.lagr_multipliers)):\n",
    "                    prediction += self.lagr_multipliers[i]*self.support_vector_labels[i] *\\\n",
    "                    self.kernel(self.support_vectors[i], sample)\n",
    "                predictions += self.intercept \n",
    "                y_pred.append(np.sign(prediction))\n",
    "            return np.array(y_pred)\n",
    "        \n",
    "        def score(self, X, y):\n",
    "            y_pred = self.predict(X)\n",
    "            accuracy = np.sum(y == y_pred, axis=0) / len(y)\n",
    "            return accuracy\n",
    "                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79cb56c-3ecb-4ed1-a9ff-8ed7ad4d2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "class KNN():\n",
    "    \n",
    "    def __init__(self, k=10):\n",
    "        self._k = k\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self._unique_labels = np.unique(y)\n",
    "        self._class_num = len(self._unique_labels)\n",
    "        self._datas = X\n",
    "        self._labels = y.astype(np.int32)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        dist = np.sum(np.square(X), axis=1, keepdims = True) - 2 * np.dot(X, self._datas.T)\n",
    "        dist = dist + np.sum(np.square(self._datas), axis=1, keepdims=True).T\n",
    "        dist = np.argsort(dist)[:, :self._k]\n",
    "        return np.array([np.argmax(np.bincount(self._labels[dist][i])) for i in range(len(X))])\n",
    "        idx = lagr_mult > 1e-7\n",
    "        self.lagr_multipliers = lagr_mult[idx]\n",
    "        self.support_vectors = X[idx]\n",
    "        \n",
    "        self.intercept = self.support_vector_labels[0]\n",
    "        for i in range(len(self.lagr_multipliers)):\n",
    "            self.intercept -= self.lagr_mutipliers[i] * self.support_vector_labels[i] * \\\n",
    "            self.kernel(self.support_vectors[i], self.support_vectors[0])\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for sasmple in X:\n",
    "            prediction = 0\n",
    "            for i in range(len(self.lagr_multipliers)):\n",
    "                prediction += self.lagr_multipliers[i] * self.support_vector_labels[i] * \\\n",
    "                self.kernel(self.support_vectors[i], sample)\n",
    "            prediction += self.intercept\n",
    "            y_pred.append(np.sign(prediction))\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.sum(y==y+pred, axis=0) / len(y)\n",
    "        return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90feac98-e35c-4247-9310-da2e9814e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \n",
    "    def __init__(self, feature_i=None, threshold=None, value=None, true_branch\n",
    "                 =None, false_branch=None):\n",
    "        self.feature_i = feature_i\n",
    "        self.threshold=threshold\n",
    "        self.value = value\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "        \n",
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "        \n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "    \n",
    "    return np.array([X_1, X_2])\n",
    "\n",
    "class DecisionTree(object):\n",
    "    \n",
    "    def __init__(self, min_sample_split = 2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        self._impurity_calculation = None\n",
    "        self._leaf_value_calculation = None\n",
    "        self.one_dim = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        self.root = self._build_tree(X, y)\n",
    "        \n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        largest_impurity = 0\n",
    "        best_criteria = None\n",
    "        best_sets = None\n",
    "        \n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expend_dims(y, axis=1)\n",
    "            \n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "        \n",
    "        n_samples, n_features = np.shape(X)\n",
    "        \n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            for feature_i in range(n_features):\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values) \n",
    "                \n",
    "                for threshold in unique_values:\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "                    \n",
    "                if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                    y1 = Xy1[:, n_features:]\n",
    "                    y2 = Xy2[:, n_features:]\n",
    "                    \n",
    "                    impurity = self._impurity_calculation(y, y1, y2)\n",
    "                    \n",
    "                    if impurity > largest_impurity:\n",
    "                        largest_impurity = impurity\n",
    "                        best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
    "                        best_sets = {\n",
    "                            \"leftX\": Xy1[:, :n_features], \n",
    "                            \"lefty\": Xy1[:, n_features:],\n",
    "                            \"rightX\": Xy2[:, :n_features],\n",
    "                            \"righty\": Xy2[:, n_features:]\n",
    "                        }\n",
    "        if largest_impurity > self.min_impurity:\n",
    "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth +1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth+1)\n",
    "            return DecisionNode(feature_i = best_criteria[\"feature_i\"], threshold=best_criteria[\"threshold\"],\n",
    "                                true_branch = true_branch, false_branch = false_branch)\n",
    "        \n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "        return DecisionNode(value = leaf_value)\n",
    "    \n",
    "    def predict_value(self, x, tree=None):\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature_i]\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "        \n",
    "        return self.predict_value(x, branch)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.sum(y==y_pred, axis=0) / len(y)\n",
    "        return accuracy\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "            \n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "        else:\n",
    "            print(\"feature|threshold -> %s | %s\" % (tree.feature_i, tree.threshold))\n",
    "            print(\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.true_branch, indent + indent)\n",
    "            print(\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.false_branch, indent + indent)\n",
    "            \n",
    "def calculate_entropy(y):\n",
    "    log2 = lambda x: math.log(x) / math.log(2) \n",
    "    unique_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in unique_labels:\n",
    "        count = len(y[y==label])\n",
    "        p = count / len(y)\n",
    "        entropy += -p * log2(p)\n",
    "    return entropy\n",
    "\n",
    "def calculate_gini(y):\n",
    "    unique_labels = np.unique(y)\n",
    "    var = 0\n",
    "    for label in unique_labels:\n",
    "        count = len(y[y==label])\n",
    "        p = count / len(y)\n",
    "        var += p**2\n",
    "    return 1-var\n",
    "\n",
    "\n",
    "    \n",
    "                                           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002209b2-32c6-4399-a864-e3b9c87b872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree(DecisionTree):\n",
    "    def _calculate_gini_index(self, y, y1, y2):\n",
    "        p = len(y1) / len(y)\n",
    "        gini = calculate_gini(y)\n",
    "        gini_index = gini-p*calculate_gini(y1) - (1-p)* calculate_gini(y2)\n",
    "        return gini_index\n",
    "    \n",
    "    def _calculate_information_gain(self, y, y1, y2):\n",
    "        p = len(y1) / len(y)\n",
    "        entropy = calculate_entropy(y)\n",
    "        info_gain = entropy - p*calculate_entropy(y1) - (1-p)*calculate_entropy(y2)\n",
    "        return info_gain\n",
    "    def _majority_vote(self, y):\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            count = len(y[y==label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count \n",
    "        return most_common\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_gini_index\n",
    "        self._leaf_value_caculation = self._majority_vote\n",
    "        super(ClassificationTree, self).fit(X,y)\n",
    "        \n",
    "def calculate_mse(y):\n",
    "    return np.mean((y-np.mean(y))**2)\n",
    "\n",
    "def calculate_variance(y):\n",
    "    n_samples = np.shape(y)[0]\n",
    "    variance = (1/n_sample)*np.diag((y-np.mean(y)).T.dot(y-np.mean(y)))\n",
    "    return variance \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "914c405a-35ad-4cb8-8284-61139baf986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(DecisionTree):\n",
    "    def _calculate_mse(self, y, y1, y2):\n",
    "        mse_tot = calculate_mse(y)\n",
    "        mse_1 = calculate_mse(y1)\n",
    "        mse_2 = calculate_mse(y2)\n",
    "        frac_1 = len(y1) / len(y)\n",
    "        frac_2 = len(y2) / len(y)\n",
    "        mse_reduction = mse_tot - (frac_1*mse_1+frac_2*mse_2)\n",
    "        return mse_reduction\n",
    "    \n",
    "    def _calculate_variance_reduction(self, y, y1, y2):\n",
    "        var_tot = calculate_variance(y)\n",
    "        var_1 = calculate_variance(y1)\n",
    "        var_2 = calculate_variance(y2)\n",
    "        frac_1 = len(y1) / len(y)\n",
    "        frac_2 = len(y2) / len(y)\n",
    "        variance_reduction = var_tot - (frac_1*var_1+frac_2*var_2)\n",
    "        return sum(variance_reduction)\n",
    "    \n",
    "    def _mean_of_y(self, y):\n",
    "        value = np.mean(y, axis=0)\n",
    "        return value if len(value) > 1 else value[0]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_mse\n",
    "        self._leaf_value_calculation = self._mean_of_y\n",
    "        super(RegressionTree, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47c3532-2136-4854-bdcb-1283b4c2192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, n_components):\n",
    "        n_sample = np.shape(X)[0]\n",
    "        covariance_matrix = (1/(n_samples-1))*(X-X.mean(axis=0)).T.dot(X-X.mean(axis=0))\n",
    "        \n",
    "        eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "        \n",
    "        eigenvalues = eigenvalues[idx][:n_components]\n",
    "        eigenvectors = np.atleast_1d(eigenvectors[:,idx])[:,:n_components]\n",
    "        \n",
    "        X_transformed = X.dot(eigenvectors)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ab3eae-fc53-440f-955d-67b7df79553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans \n",
    "\n",
    "def distEclud(x,y):\n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "def randomCent(dataSet, k):\n",
    "    m,n = dataSet.shape\n",
    "    centroids = np.zeros((k,n))\n",
    "    for i in range(k):\n",
    "        index = int(np.random.uniform(0,m))\n",
    "        centroids[i,:] = dataSet[index,:]\n",
    "    return centroids\n",
    "\n",
    "class KMeans():\n",
    "    def __init__(self):\n",
    "        self.dataSet = None\n",
    "        self.k = None\n",
    "    def fit(self, dataSet, k):\n",
    "        self.dataSet = dataSet\n",
    "        self.k = k\n",
    "        m = np.shape(dataSet)[0]\n",
    "        clusterAssment = np.mat(np.zeros((m,2)))\n",
    "        clusterChange = True\n",
    "        centroids = randomCent(self.dataSet, k)\n",
    "        while clusterChange:\n",
    "            culsterChange = False\n",
    "            for i in range(m):\n",
    "                minDist = 1e6\n",
    "                minIndex = -1\n",
    "                for j in range(k):\n",
    "                    distance = distEclud(centroids[j,:], self.dataSet[i,:])\n",
    "                    if distance < minDist:\n",
    "                        minDist = distance \n",
    "                        minIndex = j\n",
    "                if clusterAssment[i,0] != minIndex:\n",
    "                    clusterChange = True\n",
    "                    clsuterAssment[i,:] = minIndex, minDist ** 2\n",
    "            for j in range(k):\n",
    "                pointsInCluster = dataSet[np.nonzero(clusterAssment[:,0].A == j)[0]]\n",
    "                centroids[j,:] = np.mean(pointsInCluster, axis=0)\n",
    "                \n",
    "        return centroids, clusterAssment\n",
    "        \n",
    "        return X_transformed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ad81a-b487-4b8a-9f20-d198498c29db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713fdb4-a785-4179-b103-7467d0eab8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7ab3b-f83b-4fbf-9dd1-b22796237bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eed17b-b443-4004-acfa-95295cc6c24d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
